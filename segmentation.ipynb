{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b08717c9-1d24-4f04-be2d-07eebb455ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `keras` framework.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import clear_output\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from tensorflow.keras.applications.densenet import DenseNet201\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c06bbea-a854-4ac0-ab81-3a66fa4cdb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(\"/home/lusk.c/XN_project/images_segmentation.npy\")\n",
    "y = np.load(\"/home/lusk.c/XN_project/masks_segmentation.npy\")\n",
    "x = x/255\n",
    "x = np.expand_dims(x, axis = 3)\n",
    "y = np.expand_dims(y, axis = 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263c5f87-bc73-40d7-80e9-f3f5e8bb4a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "x2 = np.zeros((x.shape[0],512,512,3), dtype='float32')\n",
    "while n < x.shape[0]:\n",
    "    x2[n] = cv2.merge((x[n],x[n],x[n]))\n",
    "    n +=1\n",
    "x = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ee778c-b407-4f9c-938d-aa5f830e9493",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 0\n",
    "y2 = np.zeros((y.shape[0],512,512,3), dtype='float32')\n",
    "while n < y.shape[0]:\n",
    "    y2[n] = cv2.merge((y[n],y[n],y[n]))\n",
    "    n +=1\n",
    "y = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347add18-85e5-40b2-8836-6d3d821e2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X,train_y, val_y = train_test_split(x2,y2,test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6161304e-c64f-4ce4-b393-22bedb8357ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image size that we are going to use\n",
    "IMG_SIZE = 512\n",
    "# Our images are RGB (3 channels)\n",
    "N_CHANNELS = 1\n",
    "# Scene Parsing has 150 classes + `not labeled`\n",
    "N_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f348379-de91-477c-ae85-25149fe86b93",
   "metadata": {},
   "outputs": [],
   "source": [
    " base = keras.applications.DenseNet121(input_shape=[512,512,3], \n",
    "                                       include_top=False, \n",
    "                                       weights='imagenet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f6ba658-dc54-4f68-bc8a-7fa1efd5f3df",
   "metadata": {},
   "outputs": [],
   "source": [
    " skip_names = ['conv1/relu', # size 64*64\n",
    "              'pool2_relu',  # size 32*32\n",
    "              'pool3_relu',  # size 16*16\n",
    "              'pool4_relu',  # size 8*8\n",
    "              'relu'        # size 4*4\n",
    "              ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "877c5be2-01ca-4333-a9b4-e7d6b1cb8962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 256, 256, 64), dtype=tf.float32, name=None), name='conv1/relu/Relu:0', description=\"created by layer 'conv1/relu'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 128, 128, 256), dtype=tf.float32, name=None), name='pool2_relu/Relu:0', description=\"created by layer 'pool2_relu'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 64, 64, 512), dtype=tf.float32, name=None), name='pool3_relu/Relu:0', description=\"created by layer 'pool3_relu'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 32, 32, 1024), dtype=tf.float32, name=None), name='pool4_relu/Relu:0', description=\"created by layer 'pool4_relu'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 16, 16, 1024), dtype=tf.float32, name=None), name='relu/Relu:0', description=\"created by layer 'relu'\")\n"
     ]
    }
   ],
   "source": [
    " skip_outputs = [base.get_layer(name).output for name in skip_names]\n",
    " for i in range(len(skip_outputs)):\n",
    "     print(skip_outputs[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d9ae85-5254-46d8-b406-69a8eaec89f2",
   "metadata": {},
   "outputs": [],
   "source": [
    " downstack = keras.Model(inputs=base.input,\n",
    "                        outputs=skip_outputs)\n",
    " downstack.trainable = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "019c1294-c6a0-4278-b10f-5ff80d097bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    " # Four upstack blocks for upsampling sizes \n",
    " # 4->8, 8->16, 16->32, 32->64 \n",
    "upstack = [pix2pix.upsample(512,3),\n",
    "           pix2pix.upsample(256,3),\n",
    "           pix2pix.upsample(128,3),\n",
    "           pix2pix.upsample(64,3)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66705c3a-845a-4cf3-9469-cafd4e7d853a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input layer\n",
    "inputs = keras.layers.Input(shape=[512,512,3])\n",
    " # downsample \n",
    "down = downstack(inputs)\n",
    "out = down[-1]\n",
    "# prepare skip-connections\n",
    "skips = reversed(down[:-1])\n",
    "# choose the last layer at first 4 --> 8\n",
    "# upsample with skip-connections\n",
    "for up, skip in zip(upstack,skips):\n",
    "    out = up(out)\n",
    "    out = keras.layers.Concatenate()([out,skip])\n",
    "# define the final transpose conv layer\n",
    "# image 128 by 128 with 59 classes\n",
    "out = keras.layers.Conv2DTranspose(3, 3,\n",
    "                                   strides=2,\n",
    "                                   padding='same',\n",
    "                                   name=None,\n",
    "                                   )(out)\n",
    " # complete unet model\n",
    "unet = keras.Model(inputs=inputs, outputs=out) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2863377-db9c-4555-8f4a-82fd47dcccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = tf.data.Dataset.from_tensor_slices(train_X)\n",
    "val_X = tf.data.Dataset.from_tensor_slices(val_X)\n",
    "train_y = tf.data.Dataset.from_tensor_slices(train_y)\n",
    "val_y = tf.data.Dataset.from_tensor_slices(val_y)\n",
    "train_X.element_spec, train_y.element_spec, val_X.element_spec, val_y.element_spec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e01e5-c5b4-4adf-9ca1-3e9c0b417650",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.zip((train_X, train_y))\n",
    "val = tf.data.Dataset.zip((val_X, val_y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea713f-a554-4c02-9233-e4eb7b8e6664",
   "metadata": {},
   "outputs": [],
   "source": [
    " def brightness(img, mask):\n",
    "    img = tf.image.adjust_brightness(img, 0.1)\n",
    "    return img, mask\n",
    " \n",
    "def gamma(img, mask):\n",
    "    img = tf.image.adjust_gamma(img, 0.1)\n",
    "    return img, mask\n",
    "def hue(img, mask):\n",
    "     img = tf.image.adjust_hue(img, -0.1)\n",
    "     return img, mask\n",
    "\n",
    "def crop(img, mask):\n",
    "    img = tf.image.central_crop(img, 0.7)\n",
    "    img = tf.image.resize(img, (128,128))\n",
    "    mask = tf.image.central_crop(mask, 0.7)\n",
    "    mask = tf.image.resize(mask, (128,128))\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    return img, mask\n",
    "\n",
    "def flip_hori(img, mask):\n",
    "    img = tf.image.flip_left_right(img)\n",
    "    mask = tf.image.flip_left_right(mask)\n",
    "    return img, mask\n",
    "\n",
    "def flip_vert(img, mask):\n",
    "    img = tf.image.flip_up_down(img)\n",
    "    mask = tf.image.flip_up_down(mask)\n",
    "    return img, mask\n",
    "\n",
    "def rotate(img, mask):\n",
    "    img = tf.image.rot90(img)\n",
    "    mask = tf.image.rot90(mask)\n",
    "    return img, mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c462f0db-9d7d-46ad-b55b-2b9e39a910f5",
   "metadata": {},
   "outputs": [],
   "source": [
    " # perform augmentation on train data only\n",
    "b = train.map(flip_hori)\n",
    "\n",
    "train = train.concatenate(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c78b577-5b6e-4d8a-a0f6-916087a022f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH = 64\n",
    "AT = tf.data.AUTOTUNE\n",
    "BUFFER = 1000\n",
    "STEPS_PER_EPOCH = 800//BATCH\n",
    "VALIDATION_STEPS = 200//BATCH\n",
    "train = train.cache().shuffle(BUFFER).batch(BATCH).repeat()\n",
    "train = train.prefetch(buffer_size=AT)\n",
    "val = val.batch(BATCH) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd49ee01-f3a2-4bd5-a79f-48dbea743a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "def dice_coeff(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    score = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return score\n",
    "\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    loss = 1 - dice_coeff(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    loss = binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "    return loss\n",
    "def lr_scheduler(epoch):\n",
    "    if epoch < 22:\n",
    "        return 0.1\n",
    "    elif epoch < 44:\n",
    "        return .01\n",
    "    elif epoch < 80:\n",
    "        return 0.001\n",
    "    else:\n",
    "        return .000001\n",
    "scheduler = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=10, # how many epochs to wait before stopping\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83b146e-428e-4fe6-8789-008f35f59c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet.compile(loss= bce_dice_loss,\n",
    "             optimizer=keras.optimizers.RMSprop(learning_rate=0.1),\n",
    "             metrics=dice_coeff) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0d3f9-e1e5-4aaa-9c6c-b49d62365159",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = unet.fit(train,\n",
    "                validation_data=val,\n",
    "                steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                validation_steps=VALIDATION_STEPS,\n",
    "                epochs=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf66383-2872-4eae-b3f9-ac673290bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, mask = next(iter(val))\n",
    "pred = unet.predict(img)\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "for i in pred:\n",
    "    plt.subplot(121)\n",
    "    i = tf.argmax(i, axis=-1)\n",
    "    plt.imshow(i,cmap='jet')\n",
    "    plt.axis('off')\n",
    "    plt.title('Prediction')\n",
    "    break\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(mask[0], cmap='jet')\n",
    "plt.axis('off')\n",
    "plt.title('Ground Truth')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e00670-d15c-4184-b514-6e83086ae9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i + 1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img\n",
    "        (display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    for image, mask in dataset.take(num):\n",
    "        pred_mask = unet.predict(image)\n",
    "        pred_mask *= 255.0\n",
    "        print(pred_mask.min())\n",
    "        print(pred_mask.max())\n",
    "        print(np.unique(pred_mask, return_counts=True))\n",
    "        display([image[0], mask[0], pred_mask[0]])\n",
    "\n",
    "show_predictions(val, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b2f433-5357-4231-99d7-5d38c892d288",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
